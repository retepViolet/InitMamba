{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxjGiug1DmSI"
   },
   "outputs": [],
   "source": [
    "%pip install mambapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3024,
     "status": "ok",
     "timestamp": 1734936531480,
     "user": {
      "displayName": "EN WEI",
      "userId": "03929691894706464420"
     },
     "user_tz": -480
    },
    "id": "pEzrz04BDK03",
    "outputId": "12af59dc-1304-4fd1-a45f-650781252ca8"
   },
   "outputs": [],
   "source": [
    "import torch, transformers, importlib, InitMamba\n",
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[14569,   310,  6926,    15,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "inputs = tokenizer(\"Today is Sunday.\", return_tensors=\"pt\", max_length=10, padding='max_length')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(InitMamba)\n",
    "import InitMamba\n",
    "\n",
    "config = MambaConfig.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "config.use_mambapy = True\n",
    "model1 = InitMamba.MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\", config=config) #.to('cuda')\n",
    "# model1.train()\n",
    "\n",
    "res1 = model1(**inputs, output_ssm_last_states = True)\n",
    "ssm_last_states = res1.ssm_last_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[', the,\\n<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']\n",
      "[' is Sunday,\\n<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "# Model without initial SSM states gives random results\n",
    "print(tokenizer.batch_decode(res1.logits.argmax(dim=-1)))\n",
    "\n",
    "# Model with initial SSM states gives \"Sunday\"\n",
    "res2 = model1(**inputs, inputs_ssm_states = ssm_last_states)\n",
    "print(tokenizer.batch_decode(res2.logits.argmax(dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model2 = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\", config=config)\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "print((model2(**inputs).logits - model1(**inputs).logits).abs().sum())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
